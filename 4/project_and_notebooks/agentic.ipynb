{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e80efe7",
   "metadata": {},
   "source": [
    "# ðŸ¤– AI Agent with OpenAI & Tavily\n",
    "This notebook covers the AI functions that interact with **Tavily** for search and **OpenAI** for response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337da57",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Setting Up Tavily & OpenAI\n",
    "Tavily is a search engine that retrieves up-to-date information, while OpenAI's models generate human-like responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2458da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = os.path.join(os.path.dirname(__file__), '.env')\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "atavily_client = AsyncTavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f2302",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Simple LLM Response\n",
    "This function directly calls the OpenAI API to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e79755",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simple_response(prompt: str, model: ChatOpenAI = llm):\n",
    "    try:\n",
    "        response = await model.ainvoke(prompt)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        return 'Error in response generation'\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894f3c6",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Search & AI Integration\n",
    "This function first queries Tavily, then summarizes the results using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72175e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_info(query: str):\n",
    "    result = await atavily_client.search(query)\n",
    "    info = '\\n'.join([r['content'] for r in result['results']])\n",
    "    response = await llm.ainvoke(f'Explain the following information: {info}')\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60da57",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Testing the AI Agent\n",
    "Let's test our agent by querying information from Tavily and getting an AI-generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "query = 'Who is Dimebag Darrell?'\n",
    "output = asyncio.run(get_info(query))\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
