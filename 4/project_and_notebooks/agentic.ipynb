{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ¤– AI Agent with OpenAI & Tavily\n",
    "This notebook covers the AI functions that interact with **Tavily** for search and **OpenAI** for response generation."
   ],
   "id": "4e80efe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Everyone knows OpenAI nowadays! They have a Langchain integration of their API so that we don't explicitly need to adjust our approach to OpenAI API structure. The same way we could use Anthropic, Gemini, all other models without big difference because the interface is the same! Just run ```.invoke()``` Langchain method!",
   "id": "e7d2c82ad9ecb3bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In general terms, OpenAI provides more capabilities than purely LLM calling. One of the examples are scheduled tasks, text embedding for RAG, model finetuning. Here we will only cover large language model invocation. https://platform.openai.com/",
   "id": "86e2f858357725bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tavily is a service that provides an LLM-optimized web search for your agents. It can not only work with textual data, but it can also return pictures and other objects, and has a lot different cabalities, but we will mainly cover basics here. https://tavily.com/",
   "id": "b724aedc68da5be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Œ Setting Up Tavily & OpenAI\n",
    "Here we define our classes and setup environmental variables."
   ],
   "id": "3337da57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# all integrations can be found here: https://python.langchain.com/docs/integrations/chat/\n",
    "# they all have mostly the same interface due to Langchain, so swapping them is not a big problem\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = os.path.join(os.path.dirname(__file__), '.env')\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])  # AsyncTavilyClient() for async\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ],
   "id": "5e2458da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Œ Simple LLM Response\n",
    "Let's make a simple function that just queries the LLM."
   ],
   "id": "555f2302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simple_response(prompt: str):\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ],
   "id": "31e79755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prompt = \"Hey! How are you doing?\"",
   "id": "a3458f0c1c90b6db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's print it.",
   "id": "91d2da23c29769a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(simple_response(prompt))",
   "id": "5ca08a6ffbd8b01e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Œ Search & AI Integration\n",
    "This is all fun, of course, but how do we level up?"
   ],
   "id": "a894f3c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is where Tavily comes in. Now, instead of purely calling an LLM without any agentic capabilities, we add a search engine to it.",
   "id": "fbb018b8ec2d5324"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_info(query: str):\n",
    "    result = tavily_client.search(query)\n",
    "    info = '\\n'.join([r['content'] for r in result['results']])\n",
    "    response = llm.invoke(f'Explain the following information: {info}')\n",
    "    return response.content"
   ],
   "id": "72175e51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Œ Testing the AI Agent\n",
    "Let's test our agent by querying information from Tavily and getting an AI-generated response."
   ],
   "id": "9f60da57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = 'Who is Dimebag Darrell?'\n",
    "output = get_info(query)\n",
    "print(output)"
   ],
   "id": "a8a7bb33"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
